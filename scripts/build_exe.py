#!/usr/bin/env python
"""Build standalone executable with all dependencies and models for offline use.

This creates a self-contained package that can run without Python or network.

Usage:
    python scripts/build_exe.py

Output:
    dist/rag/         - Folder with executable and all dependencies
    dist/rag.zip      - Zipped distribution (optional)
"""
import os
import shutil
import subprocess
import sys
from pathlib import Path

ROOT_DIR = Path(__file__).parent.parent
DIST_DIR = ROOT_DIR / "dist"
BUILD_DIR = ROOT_DIR / "build"
MODELS_DIR = ROOT_DIR / "models"


def download_models():
    """Download all required models for offline use."""
    print("\n" + "=" * 60)
    print("Step 1: Downloading models for offline use...")
    print("=" * 60 + "\n")

    # Temporarily disable offline mode
    os.environ.pop("HF_HUB_OFFLINE", None)
    os.environ.pop("TRANSFORMERS_OFFLINE", None)

    # Set cache to local models folder
    os.environ["HF_HOME"] = str(MODELS_DIR / ".cache")
    os.environ["TRANSFORMERS_CACHE"] = str(MODELS_DIR / ".cache")

    # Download embedding model
    print("Downloading embedding model...")
    try:
        from sentence_transformers import SentenceTransformer
        model = SentenceTransformer(
            "sentence-transformers/paraphrase-multilingual-mpnet-base-v2",
            cache_folder=str(MODELS_DIR / "sentence-transformers")
        )
        print(f"  Embedding model saved to: {MODELS_DIR / 'sentence-transformers'}")
    except Exception as e:
        print(f"  Warning: Could not download embedding model: {e}")

    # Download Docling models (if any)
    print("\nDownloading Docling models...")
    try:
        # Docling downloads models on first use
        # We trigger this by importing
        from docling.document_converter import DocumentConverter
        print("  Docling models will be downloaded on first document conversion")
    except Exception as e:
        print(f"  Warning: {e}")

    print("\nModels ready for bundling.")


def create_spec_file():
    """Create PyInstaller spec file."""
    print("\n" + "=" * 60)
    print("Step 2: Creating PyInstaller spec file...")
    print("=" * 60 + "\n")

    # Collect data files
    datas = []

    # Add models directory
    if MODELS_DIR.exists():
        datas.append(f"('{MODELS_DIR}', 'models')")
        print(f"  Including models: {MODELS_DIR}")

    # Add config directory
    config_dir = ROOT_DIR / "config"
    if config_dir.exists():
        datas.append(f"('{config_dir}', 'config')")
        print(f"  Including config: {config_dir}")

    # Check for HuggingFace cache
    hf_cache = MODELS_DIR / ".cache"
    if hf_cache.exists():
        datas.append(f"('{hf_cache}', 'models/.cache')")
        print(f"  Including HF cache: {hf_cache}")

    datas_str = ",\n        ".join(datas)

    spec_content = f'''# -*- mode: python ; coding: utf-8 -*-
# Generated by build_exe.py
# Includes all models and dependencies for offline operation

import sys
from pathlib import Path
from PyInstaller.utils.hooks import collect_data_files, collect_submodules

block_cipher = None

# Collect all data files from key packages
torch_datas = collect_data_files('torch')
transformers_datas = collect_data_files('transformers')
sentence_transformers_datas = collect_data_files('sentence_transformers')
chromadb_datas = collect_data_files('chromadb')
docling_datas = collect_data_files('docling')

# Collect submodules
torch_hiddenimports = collect_submodules('torch')
transformers_hiddenimports = collect_submodules('transformers')
sentence_transformers_hiddenimports = collect_submodules('sentence_transformers')

a = Analysis(
    ['{ROOT_DIR / "cli" / "cli.py"}'],
    pathex=['{ROOT_DIR}'],
    binaries=[],
    datas=[
        {datas_str},
    ] + torch_datas + transformers_datas + sentence_transformers_datas + chromadb_datas + docling_datas,
    hiddenimports=[
        # Core
        'typer',
        'typer.main',
        'rich',
        'rich.console',
        'rich.table',
        'rich.progress',
        'click',

        # Storage
        'chromadb',
        'chromadb.api',
        'chromadb.config',
        'chromadb.db',
        'sqlite3',

        # ML/AI
        'torch',
        'torch.nn',
        'torch.cuda',
        'transformers',
        'sentence_transformers',
        'sentence_transformers.models',

        # Document processing
        'docling',
        'docling.document_converter',
        'pypdf',
        'langdetect',

        # MCP Server
        'fastmcp',
        'uvicorn',
        'starlette',
        'httpx',

        # Utilities
        'pydantic',
        'pydantic_settings',
        'rank_bm25',
        'tqdm',
        'numpy',

        # Our modules
        'src',
        'src.config',
        'src.models',
        'src.storage',
        'src.storage.chroma_client',
        'src.storage.bm25',
        'src.ingestion',
        'src.ingestion.pipeline',
        'src.ingestion.parallel_pipeline',
        'src.ingestion.checkpoint',
        'src.ingestion.status',
        'src.ingestion.lock',
        'src.ingestion.audit_log',
        'src.mcp',
        'src.mcp.server',
        'src.mcp.status',
        'src.project',
        'src.query',
        'src.utils',
        'cli',
        'cli.cli',
    ] + torch_hiddenimports + transformers_hiddenimports + sentence_transformers_hiddenimports,
    hookspath=[],
    hooksconfig={{}},
    runtime_hooks=[],
    excludes=[
        'tkinter',
        'matplotlib',
        'notebook',
        'jupyter',
        'IPython',
        'pytest',
        'sphinx',
    ],
    win_no_prefer_redirects=False,
    win_private_assemblies=False,
    cipher=block_cipher,
    noarchive=False,
)

pyz = PYZ(a.pure, a.zipped_data, cipher=block_cipher)

# Create a folder distribution (not single file) to handle large models
exe = EXE(
    pyz,
    a.scripts,
    [],
    exclude_binaries=True,
    name='rag',
    debug=False,
    bootloader_ignore_signals=False,
    strip=False,
    upx=False,  # Disable UPX for faster startup
    console=True,
    disable_windowed_traceback=False,
    argv_emulation=False,
    target_arch=None,
    codesign_identity=None,
    entitlements_file=None,
    icon=None,
)

coll = COLLECT(
    exe,
    a.binaries,
    a.zipfiles,
    a.datas,
    strip=False,
    upx=False,
    upx_exclude=[],
    name='rag',
)
'''

    spec_file = ROOT_DIR / "rag_offline.spec"
    spec_file.write_text(spec_content)
    print(f"  Spec file created: {spec_file}")
    return spec_file


def run_pyinstaller(spec_file):
    """Run PyInstaller with the spec file."""
    print("\n" + "=" * 60)
    print("Step 3: Running PyInstaller (this may take several minutes)...")
    print("=" * 60 + "\n")

    # Install PyInstaller if needed
    subprocess.run([sys.executable, "-m", "pip", "install", "pyinstaller>=6.0"], check=True)

    # Clean previous builds
    if BUILD_DIR.exists():
        shutil.rmtree(BUILD_DIR)
    if (DIST_DIR / "rag").exists():
        shutil.rmtree(DIST_DIR / "rag")

    # Run PyInstaller
    result = subprocess.run([
        sys.executable, "-m", "PyInstaller",
        "--clean",
        "--noconfirm",
        str(spec_file)
    ], cwd=str(ROOT_DIR))

    return result.returncode == 0


def create_launcher():
    """Create a launcher batch file for the distribution."""
    print("\n" + "=" * 60)
    print("Step 4: Creating launcher scripts...")
    print("=" * 60 + "\n")

    dist_rag = DIST_DIR / "rag"

    # Create Windows batch launcher
    launcher_bat = dist_rag / "rag.bat"
    launcher_bat.write_text('''@echo off
REM Docling-RAG Launcher
REM Add this folder to PATH or run rag.exe directly

"%~dp0rag.exe" %*
''')
    print(f"  Created: {launcher_bat}")

    # Create README for distribution
    readme = dist_rag / "README.txt"
    readme.write_text('''Docling-RAG - Standalone Distribution
=====================================

This is a self-contained distribution that includes:
- All Python dependencies
- Embedding models
- Document processing libraries

INSTALLATION
------------
1. Extract this folder to a permanent location (e.g., C:\\Program Files\\docling-rag)
2. Add the folder to your PATH environment variable
3. Run 'rag --help' to verify installation

QUICK START
-----------
1. Initialize: rag init
2. Add documents: rag ingestion start C:\\path\\to\\docs
3. Query: rag query "your question"
4. Start MCP server: rag mcp serve

For more information: rag --help

OFFLINE OPERATION
-----------------
This distribution includes all required models and can run
completely offline without internet access.
''')
    print(f"  Created: {readme}")


def calculate_size():
    """Calculate and report distribution size."""
    dist_rag = DIST_DIR / "rag"

    if not dist_rag.exists():
        return

    total_size = sum(f.stat().st_size for f in dist_rag.rglob("*") if f.is_file())
    size_mb = total_size / (1024 * 1024)
    size_gb = total_size / (1024 * 1024 * 1024)

    print("\n" + "=" * 60)
    print("Build Complete!")
    print("=" * 60)
    print(f"\nDistribution folder: {dist_rag}")
    print(f"Total size: {size_mb:.1f} MB ({size_gb:.2f} GB)")
    print(f"\nTo install:")
    print(f"  1. Copy '{dist_rag}' to destination (e.g., C:\\Program Files\\docling-rag)")
    print(f"  2. Add to PATH")
    print(f"  3. Run: rag --help")


def main():
    print("\n" + "=" * 60)
    print("  Docling-RAG Standalone Build")
    print("  (Offline-capable distribution with bundled models)")
    print("=" * 60)

    # Step 1: Download models
    download_models()

    # Step 2: Create spec file
    spec_file = create_spec_file()

    # Step 3: Run PyInstaller
    if not run_pyinstaller(spec_file):
        print("\nERROR: PyInstaller failed!")
        sys.exit(1)

    # Step 4: Create launcher
    create_launcher()

    # Report size
    calculate_size()


if __name__ == "__main__":
    main()
